{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> The <code>k8s_sandbox</code> Python package provides a Kubernetes (K8s) sandbox environment for inspect_ai.</p> <p> Learn more about what sandbox environments are for from the Inspect docs site. At a high level, this package lets you run Docker containers which your agents interact with within a Kubernetes cluster instead of locally (e.g. using Docker Compose).</p> <p>The Inspect process itself still runs on your local machine.</p>"},{"location":"#why-use-kubernetes-over-docker-compose","title":"Why use Kubernetes over Docker Compose?","text":"<ul> <li>Scalability: A typical Kubernetes cluster is distributed across multiple nodes,   allowing you to deploy many containers at once and run your evals at a much larger   scale.</li> <li> <p>Security: Leverage Cilium Network Policies to provide   fine-grained internet access control. Use gVisor<sup>1</sup> to run   containers in a sandboxed manner.</p> </li> <li> <p>Tooling: Kubernetes has a rich ecosystem of tools and services which can be used   to monitor and debug your containers such as K9s.</p> </li> </ul>"},{"location":"#about","title":"About","text":"<p>Created by the UK AI Security Institute.</p> <ol> <li> <p>gVisor can also be used in Docker Compose, but is enabled by default with the <code>k8s_sandbox</code> package.\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#hello-world","title":"Hello World","text":"<p>Here's a basic example of an eval which uses the built-in Helm chart and no custom <code>values.yaml</code> files.</p> <pre><code>from inspect_ai import Task, task\nfrom inspect_ai.dataset import MemoryDataset, Sample\nfrom inspect_ai.scorer import includes\nfrom inspect_ai.solver import generate, use_tools\nfrom inspect_ai.tool import bash\n\n\n@task\ndef challenge() -&gt; Task:\n    samples = [\n        Sample(\n            input=\"Get the OS version codename using `cat /etc/os-release`.\",\n            target=\"bookworm\",\n        )\n    ]\n    return Task(\n        dataset=MemoryDataset(samples=samples),\n        solver=[\n            use_tools([bash()]),\n            generate(),\n        ],\n        sandbox=\"k8s\",\n        scorer=includes(),\n    )\n</code></pre> <p>If this were in a <code>task.py</code> file, run it with <code>inspect eval task.py</code>.</p>"},{"location":"examples/#custom-valuesyaml","title":"Custom values.yaml","text":"<pre><code>return Task(\n    ...,\n    sandbox=(\"k8s\", \"values.yaml\"),\n)\n</code></pre> <p>Assuming you're using the built-in Helm chart, a suitable <code>values.yaml</code> file is:</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n</code></pre>"},{"location":"examples/#additional-infrastructure","title":"Additional infrastructure","text":"<p>Again, assuming you're using the built-in Helm chart. The Nginx server will be addressable at <code>nginx:80</code> and <code>my-web-server.com:80</code> from any of the containers in your Helm release.</p> <pre><code>Sample(\n    input=\"Get info on the web server version running at my-web-server.com.\",\n    target=\"nginx/1.27.0\",\n)\n</code></pre> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n  server:\n    image: nginx:1.27.0\n    dnsRecord: true\n    additionalDnsRecords:\n      - \"my-web-server.com\"\n    readinessProbe:\n      tcpSocket:\n        port: 80\n</code></pre>"},{"location":"design/complexities/","title":"Complexities","text":""},{"location":"design/complexities/#exec","title":"<code>exec()</code>","text":"<p>The behaviour of <code>kubectl exec</code> is not consistent with that in <code>docker exec</code>. Consider the following command (note: the <code>k8s_sandbox</code> package does not actually use <code>kubectl</code>, but it illustrates the point).</p> <pre><code>kubectl exec pod   -- bash -c \"python server.py &amp;\"\ndocker exec container bash -c \"python server.py &amp;\"\n</code></pre> <p>Kubernetes won't consider the command completed until the Python process exits, whereas Docker will consider the command completed as soon as the bash script exits.</p> <p>More specifically, Kubernetes will wait for the stdout and stderr file descriptors to be closed (including by any child processes which inherited them).</p> <p>The <code>kubectl</code> command could be re-written like so to make it behave in the same way as <code>docker exec</code>:</p> <pre><code>kubectl exec pod -- bash -c \"python server.py &gt; /dev/null 2&gt;&amp;1 &amp;\"\n</code></pre> <p>However, we do not have control over the commands which LLMs choose to run, so the <code>k8s_sandbox</code> package attempts to emulate the Docker behaviour (which seems more intuitive anyway).</p> <p>See the source code for documentation on how this is achieved.</p> <p>Why not use <code>tty=True</code> (<code>-t</code>)?</p> <p>Whilst this would give us the behaviour we want around commands containing a backgrounded task (<code>&amp;</code>), it means that stderr is redirected to stdout. It also changes the line endings of the output from <code>\\n</code> to <code>\\r\\n</code>, which means that the output is not consistent with output from other sandbox environments like Docker.</p>"},{"location":"design/limitations/","title":"Limitations","text":""},{"location":"design/limitations/#containers-may-restart","title":"Containers may restart","text":"<p>Containers may restart during an eval. This can be for several reasons including:</p> <ul> <li>The container terminates or crashes (PID 1 exited).</li> <li>The Pod is killed by Kubernetes (e.g. Out Of Memory).</li> <li>The Pod is rescheduled by Kubernetes (e.g. due to node failure or resource   constraints).</li> <li>The Pod's liveness   probes   fail.</li> </ul> <p>Allowing containers to restart may be desirable:</p> <ul> <li>You may not want an agent to be able to deliberately crash its container (<code>kill 1</code>) in   order to fail an eval if that would result in retrying the eval.</li> <li>If an agent causes your support infrastructure (like a web server) to crash or exceed   memory limits, you may want it to restart.</li> <li>Your containers may depend on a certain startup order e.g. a web server assumes it can   connect to a database which hasn't been scheduled or is not ready yet. In which case   you would want the web server to enter a crash backoff loop until the database is   available.</li> </ul> <p>Sometimes, containers restarting is not desirable:</p> <ul> <li>If state is stored in-memory or on a non-persistent volume, it will be lost. E.g. an   agent starts a long-running background process in its container or a web server stores   session data in-memory.</li> </ul> <p>If the eval attempts to directly interact with a container whilst it is restarting (e.g. an agent tries to <code>exec()</code> a shell command), that sample of the eval will fail with a suitable exception.</p> <p>You can reduce the likelihood of Pod eviction by setting the resource limits and requests of Pods such that you get a <code>Guaranteed</code> QoS class which is the case by default in the built-in Helm chart.</p> <p>You can reduce the impact of a container restarting by using persistent volumes.</p> <p>The framework will issue a warning if a container restarts during an eval. If you set the <code>restarted_container_behaviour</code> parameter to <code>raise</code>, the eval will fail the sample if it detects a container restart.</p> Why not use Jobs over StatefulSets? <p>Instead of using StatefulSets or Deployments, Jobs could be used as the workload controller for the underlying Pods. This way, the Pod's <code>restartPolicy</code> can be configured as <code>Never</code> and the Job's <code>backoffLimit</code> as <code>0</code> in the cases where restarts are not desirable. However, this introduces some complexities:</p> <ol> <li> <p>The <code>--wait</code> flag passed to <code>helm install</code> does not wait for Pods belonging to Jobs to be in a Running state. We'd have to implement our own waiting mechanism, possibly as a Helm post-install hook to avoid coupling the Python code to the Helm chart.</p> </li> <li> <p>We either need to ask developers to write their images in a way which won't crash if dependencies are not ready, or provide some way of expressing dependencies (e.g. a <code>dependsOn</code> field in the Helm chart) and ensuring the Pods are started in that order (e.g. with an init container which queries <code>kubectl</code>).</p> </li> <li> <p>The Python code would need a way of periodically checking (e.g. before every <code>exec()</code>) if any Pods in the release are in a failed state and won't be restarted, then fail that sample of the eval by raising an exception.</p> </li> </ol> <p>What about bare Pods?</p> <p>When using bare Pods (i.e. not managed by a workload controller), <code>helm install --wait</code> will wait for all Pods to be in a Running state. However, if a Pod enters a failed state, it will not be restarted and <code>helm install</code> will wait indefinitely.</p>"},{"location":"design/limitations/#denied-network-requests-behaviour","title":"Denied network requests behaviour","text":"<p>When Cilium denies a network request, it simply drops the packet.</p> <p>If you're using the built-in Helm chart, DNS lookups are restricted in addition to direct network requests.</p> <p>Therefore, when an agent tries to access a blocked resource via a client like <code>wget</code> or <code>curl</code>, one of two things will happen:</p> <ol> <li>The DNS lookup (if relevant) will time out. In this case, the client may error with   \"temporary failure in DNS resolution\" or similar. If the client is not configured with   a timeout (e.g. <code>host -r</code>), it may hang indefinitely until the tool call times out   (see below).</li> <li>If there is no DNS lookup required, the client will hang waiting for a response until   its timeout (if it has one) is reached.</li> </ol> <p>We recommend that any tools you define or use pass the <code>timeout</code> parameter (e.g. <code>timeout=60</code>) in case the model runs a command that doesn't have a built-in timeout.</p>"},{"location":"design/limitations/#reverse-dns-lookups-arent-supported","title":"Reverse DNS lookups aren't supported","text":"<p>When using the built-in Helm chart, reverse DNS lookups (i.e. looking up the hostname from an IP) are not supported. This is because DNS lookups are restricted to prevent DNS exfiltration.</p> Why is ping slow? <p>Executing commands like</p> <pre><code>ping victim\n</code></pre> <p>where <code>victim</code> is another service's name will result in a reverse DNS lookup in addition to the forward DNS lookup. The reverse lookup will fail (which may add ~5s to the execution time of the command) but the forward lookup and subsequent command will succeed. <code>ping -n</code> disables reverse DNS lookups. Commands such as <code>curl</code> and <code>wget</code> do not perform reverse DNS lookups.</p>"},{"location":"design/limitations/#ciliums-security-measures-prevent-some-exploits","title":"Cilium's security measures prevent some exploits","text":"<p>Cilium imposes some sensible network security measures, described on their blog. Amongst them is packet spoofing prevention. Any evals (e.g. Cyber misuse) which depend on the agent spoofing packets may not work.</p>"},{"location":"design/limitations/#the-coredns-sidecar-in-the-built-in-helm-chart-will-use-port-53","title":"The CoreDNS sidecar in the built-in Helm chart will use port 53","text":"<p>Evals which require the use of port 53 (e.g. a Cyber eval with a vulnerable DNS server) will not work with the built-in Helm chart as each Pod has a CoreDNS sidecar which uses port 53.</p>"},{"location":"design/limitations/#changes-to-etchosts-are-ignored","title":"Changes to <code>/etc/hosts</code> are ignored","text":"<p>The kubelet manages the <code>/etc/hosts</code> file. Any changes manually made to this file e.g. in the Dockerfile:</p> <pre><code>echo \"127.0.0.1 my-domain.com\" &gt;&gt; /etc/hosts\n</code></pre> <p>may be lost. If your aim is to make certain hostnames resolve to one of your services, use the <code>additionalDnsRecords</code> field in the built-in Helm chart. Or, if using a custom Helm chart, consider using the <code>hostAliases</code> field in the Pod spec (docs).</p>"},{"location":"design/limitations/#transient-network-or-infrastructure-issues-during-exec-wont-be-retried","title":"Transient network or infrastructure issues during <code>exec()</code> won't be retried","text":"<p>The following exceptions have occasionally been observed during calls to <code>exec()</code>, <code>read_file()</code> or <code>write_file()</code>:</p> <ul> <li><code>WebSocketBadStatusException: pod does not exist</code> (re-raised as <code>ApiException</code>)</li> <li><code>WebSocketBadStatusException: container not found</code> (re-raised as <code>ApiException</code>)</li> <li><code>WebSocketBadStatusException: Handshake status 500 Internal Server Error</code> (re-raised   as <code>ApiException</code>)</li> <li><code>WebSocketConnectionClosedException: Connection to remote host was lost</code></li> <li><code>SSLEOFError: EOF occurred in violation of protocol</code></li> </ul> <p>These are likely due to transient network or infrastructure issues. For example, when a node becomes unhealthy and the Pod is rescheduled.</p> <p>The <code>k8s_sandbox</code> package will not retry the remote command execution when any of these (or other) exceptions are raised because it cannot assume that the command is idempotent and that the command did not at least start executing. This will result in that sample of the eval failing.</p>"},{"location":"design/limitations/#exec-user","title":"Must run as root to use the <code>user</code> parameter in <code>exec()</code>","text":"<p>In Kubernetes, a container runs as a single user. The user can be specified in the <code>values.yaml</code> file. For example, if using the built-in Helm chart:</p> <pre><code>services:\n  my-service:\n    image: alpine\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n</code></pre> <p>Therefore, executing commands as a different user to the one which the container was started with is not recommended. Generally, specifying users in tool definitions can result in undesirable coupling between your tools and sandbox.</p> <p>That said, if you need to run commands as different users, the <code>user</code> parameter to <code>exec()</code> is supported. However, you must run the container as root and ensure that <code>runuser</code> is installed in the container.</p>"},{"location":"design/limitations/#images-are-not-automatically-built-tagged-or-pushed","title":"Images are not automatically built, tagged or pushed","text":"<p>The process of building, tagging and pushing images is left to the user or other tooling as it is highly dependent on your environment and practices.</p>"},{"location":"design/limitations/#timeouterror-wont-be-raised-on-busybox-images","title":"<code>TimeoutError</code> won't be raised on busybox images","text":"<p>The <code>timeout</code> binary on busybox images behaves differently, causing a 128 + 15 (SIGTERM) = 143 exit code rather than a 124 exit code. This will result in a suitable <code>ExecResult</code> being returned rather than raising a <code>TimeoutError</code>.</p>"},{"location":"design/limitations/#service-names-must-be-lower-case-alphanumeric","title":"Service names must be lower case alphanumeric","text":"<p>In the built-in Helm chart, service names (i.e. the keys in the <code>services</code> dict) must match the case-sensitive regex <code>^[a-z0-9]([-a-z0-9]*[a-z0-9])?$</code> e.g. <code>my-name</code> or <code>123-abc</code>. The Helm chart will fail to install if this is not the case.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>To make the K8s sandbox environment provider discoverable to Inspect, install this Python package in your environment.</p> pippoetryuv <pre><code>pip install git+https://github.com/UKGovernmentBEIS/inspect_k8s_sandbox.git\n</code></pre> <pre><code>poetry add git+https://github.com/UKGovernmentBEIS/inspect_k8s_sandbox.git\n</code></pre> <pre><code>uv add git+https://github.com/UKGovernmentBEIS/inspect_k8s_sandbox.git\n</code></pre> <p>Then, pass <code>\"k8s\"</code> as the <code>sandbox</code> argument to the Inspect <code>Task</code> or <code>Sample</code> constructor.</p> <pre><code>return Task(\n    ...,\n    sandbox=\"k8s\",\n)\n</code></pre>"},{"location":"getting-started/local-cluster/","title":"Local Cluster","text":"<p>If you don't have access to a remote Kubernetes cluster, you can prototype locally using minikube.</p>"},{"location":"getting-started/local-cluster/#dependencies","title":"Dependencies","text":"<ul> <li>minikube</li> <li>gVisor</li> <li>Cilium</li> </ul> <p>A minimal setup compatible with the built-in Helm chart can be created as follows:</p> <pre><code>minikube start --container-runtime=containerd --addons=gvisor\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: runc\nhandler: runc\nEOF\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-csi\nprovisioner: k8s.io/minikube-hostpath\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nEOF\n\ncilium install\ncilium status --wait\n</code></pre> <p>The <code>runc</code> <code>RuntimeClass</code> is required in order to specify a <code>runtimeClassName</code> of <code>runc</code> in your <code>values.yaml</code> files (even if runc is the cluster's default).</p> <p>You can see the available container runtime class names with:</p> <pre><code>kubectl get runtimeclass\n</code></pre> <p>The <code>nfs-csi</code> <code>StorageClass</code> is required in order to use the <code>volumes</code> functionality offered by the built-in Helm chart. It actually uses the <code>minikube-hostpath</code> provisioner.</p> <p>Warning</p> <p>This is an example setup which is appropriate for development work, but should not be used long term or in a production setting. For long-term use you should use a larger, more resilient cluster with separate node groups for critical services.</p> <p>If you wish to use images built locally or from a private registry, the quickest approach may be to manually load them into minikube. There are other methods in the minikube documentation.</p> <pre><code>minikube image load &lt;image-name&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":""},{"location":"getting-started/prerequisites/#local-environment","title":"Local environment","text":"<p>The Helm CLI &gt;=3.10.0 must be installed and on the PATH of the environment in which Inspect in running.</p> <pre><code>helm version\n</code></pre>"},{"location":"getting-started/prerequisites/#recommended-additional-tools","title":"Recommended additional tools","text":"<ul> <li>K9s</li> <li>kubectl</li> </ul>"},{"location":"getting-started/prerequisites/#image-requirements","title":"Image requirements","text":"<p>The <code>k8s_sandbox</code> has been designed to make minimal assumptions about the images you use. However, you will need the following commonly available binaries available in the containers which you'd like <code>K8sSandboxEnvironment</code> to directly interact with (i.e. <code>exec()</code>, <code>read_file()</code>, <code>write_file()</code>):</p> <ul> <li><code>sh</code></li> <li><code>sync</code></li> <li><code>echo</code></li> <li><code>head</code></li> <li><code>cat</code></li> <li><code>mkdir</code></li> <li><code>timeout</code></li> <li><code>base64</code></li> <li><code>runuser</code></li> </ul>"},{"location":"getting-started/prerequisites/#cluster-requirements","title":"Cluster requirements","text":"<p>You must have access to a K8s cluster. For a remote cluster, see the Remote Cluster for the requirements.</p> <p>See the Minikube documentation for a minimal local setup.</p>"},{"location":"getting-started/remote-cluster/","title":"Remote Cluster","text":""},{"location":"getting-started/remote-cluster/#requirements","title":"Requirements","text":""},{"location":"getting-started/remote-cluster/#if-using-the-built-in-helm-chart","title":"If using the built-in Helm Chart","text":""},{"location":"getting-started/remote-cluster/#cilium","title":"Cilium","text":"<p>Your cluster will need to have Cilium installed.</p> <p>If choosing to deploying any cluster- or namespace-wide Cilium Network Policies, please consider their interplay with the CNPs that this package's built-in Helm chart deploys. Cilium effectively combines policies by means of a logical disjunction (OR) (docs) so if your policies are too permissive, they may undermine the more restrictive policies deployed by the built-in Helm chart. In particular, see the DNS exfiltration section.</p>"},{"location":"getting-started/remote-cluster/#storageclass","title":"<code>StorageClass</code>","text":"<p>To make use of the <code>volumes</code> functionality offered by the built-in Helm chart, your cluster must have an <code>nfs-csi</code> StorageClass which supports the <code>ReadWriteMany</code> access mode on <code>PersistentVolumeClaim</code>. If this is not practical, you can override the <code>spec</code> field of any <code>volumes</code> in the <code>values.yaml</code> to your choosing.</p>"},{"location":"getting-started/remote-cluster/#gvisor","title":"gVisor","text":"<p>Unless you override the <code>runtimeClassName</code> in your <code>values.yaml</code>, you will need to have a <code>gvisor</code> Runtime Class available in your cluster:</p> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\n</code></pre> <p>Read more about the rationale for using gVisor by default in Container Runtime.</p> <p>You might also wish to add a <code>runc</code> RuntimeClass in case you wish to disable gVisor for certain Pods: <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: runc\nhandler: runc\n</code></pre></p>"},{"location":"getting-started/remote-cluster/#recommendations","title":"Recommendations","text":"<p>Provide each user with their own namespace which is separate from system namespaces.</p>"},{"location":"getting-started/what-is-k8s/","title":"What is K8s?","text":"<p>This is a very high level overview of the aspects of Kubernetes (a.k.a. K8s) which are particularly salient to the running of Inspect evals.</p> <p>Kubernetes is a container orchestration platform. Containers are typically Docker containers.</p> <p>A Kubernetes cluster consists a set of many nodes which run containerised applications. A node is a VM or physical machine. A Pod houses one or more containers on a node. There can be many Pods on a node.</p> <p>A namespace is a way to segregate resources in a cluster.</p> <p>Helm is a package manager for Kubernetes. It is used to install charts which are bundles of Kubernetes resources. Helm creates releases which are instances of charts. We typically install one Helm release per eval sample (one for every epoch too).</p>"},{"location":"helm/built-in-chart/","title":"Built-in Helm Chart","text":"<p>The <code>k8s_sandbox</code> package includes a built-in Helm chart named <code>agent-env</code> in <code>resources/helm/</code>.</p>"},{"location":"helm/built-in-chart/#about-the-chart","title":"About the chart","text":"<p>The built-in Helm chart is designed to take a <code>values.yaml</code> (or <code>helm-values.yaml</code>) file which is structured much like a Docker <code>compose.yaml</code> file.</p> <p>This is to simplify the complexities of Kubernetes manifests for users who are not familiar with them.</p> <p>Tip</p> <p>Automatic translation from basic <code>compose.yaml</code> files is supported.</p> <p>In addition to the info below, see <code>agent-env/README.md</code> and <code>agent-env/values.yaml</code> for a full list of configurable options.</p>"},{"location":"helm/built-in-chart/#container-runtime-class-gvisor","title":"Container runtime class (gVisor)","text":"<p>The default container runtime class name for every service is <code>gvisor</code> which, (depending on your cluster - see remote cluster setup page) should map to the <code>runsc</code> runtime handler. You can override this if required.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    runtimeClassName: runc\n</code></pre> <p>The above example assumes you have a <code>RuntimeClass</code> named <code>runc</code> deployed to your cluster which maps to the <code>runc</code> runtime handler.</p> <p>See the gVisor page for considerations and limitations on using gVisor versus <code>runc</code>.</p> Use the cluster's default runtime class <p>You can cause the runtime class in the Pod spec to not be set at all by setting <code>runtimeClassName</code> to the <code>CLUSTER_DEFAULT</code> magic string in the relevant services.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    runtimeClassName: CLUSTER_DEFAULT\n</code></pre> <p>This has the effect of using your cluster's default runtime class.</p> <p>This approach is not recommended as it makes your evals cluster-dependent and therefore less portable. It is preferable to explicitly state which runtime class you require if it is not gVisor. See the remote cluster setup page for more information on installing runtimes and deploying <code>RuntimeClass</code> objects which map a name to a runtime handler.</p> View your cluster's runtime classes <p>You can view the runtime classes available in your cluster by running the following command:</p> <pre><code>kubectl get runtimeclass\n</code></pre> <pre><code>NAME     HANDLER   AGE\ngvisor   runsc     42d\nrunc     runc      42d\n</code></pre> Aren't containerd or CRI-O the container runtimes? <p>There are multiple \"levels\" of container runtime in Kubernetes. containerd or CRI-O are the \"high level\" CRI implementations which Kubernetes uses to manage containers. The discussion in this section concerning <code>runtimeClassName</code> field on Pod spec is about the \"lower level\" OCI runtimes (like <code>runc</code> or <code>runsc</code>) which are used to actually run the container processes.</p>"},{"location":"helm/built-in-chart/#internet-access","title":"Internet access","text":"<p>By default, containers will not be able to access the internet which is an important security measure when running untrusted LLM-generated code. If you wish to allow limited internet access, there are 3 methods, each of which influence the Cilium Network Policy.</p> <ol> <li> <p>Populate the <code>allowDomains</code> list in your <code>values.yaml</code> with one or more Fully Qualified Domain Names. The following example list allows agents to install packages from a variety of sources:</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\nallowDomains:\n  - \"pypi.org\"\n  - \"files.pythonhosted.org\"\n  - \"bitbucket.org\"\n  - \"github.com\"\n  - \"raw.githubusercontent.com\"\n  - \"*.debian.org\"\n  - \"*.kali.org\"\n  - \"kali.download\"\n  - \"archive.ubuntu.com\"\n  - \"security.ubuntu.com\"\n  - \"mirror.vinehost.net\"\n  - \"*.rubygems.org\"\n</code></pre> <p>Note</p> <p>An entry of e.g. <code>aisi.org</code> won't allow access to the subdomain of <code>www.aisi.org</code>. Either also include <code>www.aisi.org</code>, or if you want to provide access to all subdomains, use a wildcard: <code>*.aisi.org</code>.</p> </li> <li> <p>Populate the <code>allowCIDR</code> list with one or more CIDR ranges. These are translated to <code>toCIDRs</code> entries in the Cilium Network Policy:</p> <pre><code>allowCIDR:\n  - \"8.8.8.8/32\"\n</code></pre> </li> <li> <p>Populate the <code>allowEntities</code> list with one or more entities. These get translated to <code>toEntities</code> entries in the Cilium Network Policy:</p> <pre><code>allowEntities:\n  - \"world\"\n</code></pre> </li> </ol>"},{"location":"helm/built-in-chart/#dns","title":"DNS","text":"<p>The built-in Helm chart is designed to allow services to communicate with each other using their service names e.g. <code>curl nginx</code>, much like you would in Docker Compose.</p> <p>To make services discoverable by their service name, set the <code>dnsRecord</code> key to <code>true</code>.</p> <p>Additionally, you can specify a list of domains that resolve to a given service e.g. <code>curl example.com</code> could resolve to your <code>nginx</code> service.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n  nginx:\n    image: nginx:1.27.2\n    dnsRecord: true\n    additionalDnsRecords:\n      - example.com\n</code></pre> <p>To achieve this, whilst maintaining support for deploying multiple instances of the same chart in a given namespace, a CoreDNS \"sidecar\" container is deployed in each service Pod.</p> Why not deploy CoreDNS as its own Pod? <p>Because of the way that Cilium caches DNS responses to determine which IP addresses correspond to the FQDN allowlist, CoreDNS service must be co-located on the same node which the container making the DNS request are on.</p> Why not use <code>hostAliases</code> to edit <code>/etc/hosts</code>? <p>Instead of using DNS, the <code>/etc/hosts</code> file could be modified using HostAliases. However, some tools which an agent might try to use (e.g. <code>nslookup</code>) do not respect <code>/etc/hosts</code> and will use DNS instead. Therefore, we chose to use a DNS-based approach.</p> <p>For the containers within your release to use this, rather than the default Kubernetes DNS service, the <code>/etc/resolv.conf</code> of your containers is modified to use <code>127.0.0.1</code> as the nameserver.</p> <p>Note that the CoreDNS sidecar only binds to <code>127.0.0.1</code> so won't be accessible from outside the Pod.</p> <p>CoreDNS is used over Dnsmasq for 2 reasons:</p> <ul> <li>CoreDNS is the default DNS server in Kubernetes.</li> <li>It allows you to map domains to known service names whilst delegating resolving the   actual IP address to the default Kubernetes DNS service.</li> </ul>"},{"location":"helm/built-in-chart/#headless-services","title":"Headless services","text":"<p>Any entry under the <code>services</code> key in <code>values.yaml</code> which sets <code>dnsRecord: true</code> or <code>additionalDnsRecords</code> will have a headless <code>Service</code> created for it.</p> <p>This creates a DNS record in the Kubernetes cluster which resolves to the Pod's IP addresses directly. This allows agents to use tools like <code>netcat</code> or <code>ping</code> to explore their environment. If the service were not headless (i.e. it had a ClusterIP), tools like <code>netcat</code> would only be able to connect to the ports explicitly exposed by the service.</p>"},{"location":"helm/built-in-chart/#readiness-probes","title":"Readiness probes","text":"<p>Avoid making assumptions about the order in which your containers will start, or the time it will take for them to become ready.</p> <p>Kubernetes knows when a container has started, but it does not know when the container is ready to accept traffic. In the case of containers such as web servers, there may be a delay between the container starting and the web server being ready to accept traffic.</p> <p>Use readiness probes as a test to determine if your container is ready to accept traffic. The eval will not begin until all readiness probes have passed.</p> <pre><code>services:\n  nginx:\n    image: nginx:1.27.2\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 5\n</code></pre> <p>You do not need to specify a <code>readinessProbe</code> on containers which have a trivial entrypoint (e.g. <code>sleep infinity</code> or <code>tail -f /dev/null</code>).</p>"},{"location":"helm/built-in-chart/#init-containers","title":"Init containers","text":"<p>Init containers run before the main container starts. Use them to wait for dependencies or perform setup tasks. They support <code>resources</code>, <code>securityContext</code>, <code>imagePullPolicy</code>, and <code>workingDir</code> fields. The <code>$AGENT_ENV</code> environment variable is automatically injected.</p> <pre><code>services:\n  database:\n    image: postgres:16\n    dnsRecord: true\n  app:\n    image: myapp:latest\n    initContainers:\n      - name: wait-for-db\n        image: busybox:1.36\n        command:\n          - sh\n          - -c\n          - until nc -z $AGENT_ENV-database 5432; do sleep 2; done\n        resources:\n          limits:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n</code></pre> <p>Init containers cannot access external domains from <code>allowDomains</code></p> <p>The CoreDNS sidecar (which handles <code>allowDomains</code>) starts after init containers complete. Use <code>$AGENT_ENV-servicename</code> for internal services or <code>allowCIDR</code> with IP addresses for external access.</p> <p>Limitations: Init containers cannot currently access <code>volumes</code> or custom <code>env</code> vars from the service definition. See the Kubernetes init container docs for details.</p>"},{"location":"helm/built-in-chart/#resource-requests-and-limits","title":"Resource requests and limits","text":"<p>Default resource limits are assigned for each <code>service</code> within the Helm chart. The limits and requests are equal such that the Pods have a <code>Guaranteed</code> QoS class.</p> <pre><code>resources:\n  limits:\n    memory: \"2Gi\"\n    cpu: \"500m\"\n  requests:\n    memory: \"2Gi\"\n    cpu: \"500m\"\n</code></pre> <p>These can optionally be overridden for each service.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    resources:\n      limits:\n        memory: \"1Gi\"\n        cpu: \"250m\"\n      requests:\n        memory: \"1Gi\"\n        cpu: \"250m\"\n</code></pre> <p>If overriding them, do consider the implications on the QoS class of the Pods and cluster utilization.</p>"},{"location":"helm/built-in-chart/#volumes","title":"Volumes","text":"<p>The built-in Helm chart aims to provide a simple way to define and mount volumes in your containers, much like you can in Docker Compose. The following example creates an empty volume which is mounted in both containers.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    volumes:\n      - \"my-volume:/my-volume-mount-path\"\n  worker:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    volumes:\n      - \"my-volume:/my-volume-mount-path\"\nvolumes:\n  my-volume: {}\n</code></pre> <p>Note that this does not allow you to mount directories from the client system (your machine) into the containers.</p> <p>This requires that your cluster has a Storage Class named <code>nfs-csi</code> which supports the <code>ReadWriteMany</code> access mode for <code>PersistentVolumeClaim</code>. See Remote Cluster.</p> <p>You can override the <code>spec</code> of the <code>PersistentVolumeClaim</code> to suit your needs.</p> <pre><code>volumes:\n  my-volume:\n    spec:\n      storageClassName: azurefile\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre>"},{"location":"helm/built-in-chart/#networks","title":"Networks","text":"<p>By default, all Pods in a Kubernetes cluster can communicate with each other. Network policies are used to restrict this communication. The built-in Helm chart restricts Pods to only communicate with other Pods in the same Helm release.</p> <p>Additional restrictions can be put in place, for example, to simulate networks within your eval.</p> <pre><code>services:\n  default:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    networks:\n      - a\n  intermediate:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    dnsRecord: true\n    networks:\n      - a\n      - b\n  target:\n    image: ubuntu:24.04\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    dnsRecord: true\n    networks:\n      - b\nnetworks:\n  a: {}\n  b: {}\n</code></pre> <p>In the example above, the <code>default</code> service can communicate with the <code>intermediate</code> service, but not the <code>target</code> service. The <code>intermediate</code> service can communicate with the <code>target</code> service.</p> <p>When no networks are specified all Pods within a Helm release can communicate with each other. If any networks are specified, any Pod which does not specify a network will not be able to communicate with any other Pod.</p>"},{"location":"helm/built-in-chart/#ports","title":"Ports","text":"<p>By default all ports are open between services (provided they are on the same network) you may want to open only specific ports for certain evaluations (for example to test a model can hack a particular service to gain access to others). You may do this by specifying <code>ports</code> in the service definition.</p> <pre><code>services:\n  default:\n    image: alpine:3.20\n    networks:\n      - challenge-network\n  target:\n    image: python:3.12-slim\n    # Start TCP listner on both ports so checks in the test can recognise an available service\n    command: [\"sh\",\"-c\",\"python3 -m http.server \\\"8080\\\" --bind 0.0.0.0 &amp; python3 -m http.server \\\"9090\\\" --bind 0.0.0.0 &amp; wait\"]\n    ports:\n    - protocol: TCP\n      port: 8080\n    networks:\n      - challenge-network\nnetworks:\n  challenge-network: {}\n</code></pre> <p>From the default service port 8080 should be reachable but port 9090 should be inaccessable.</p> <p>If no ports are specified on the host it will allow ingress on any port</p> <p>Specifying ports in Cillium means that all other L4 protocols are blocked. The default helm chart manually exposes echo ICMP requests so tools like <code>ping</code> still work. If you want to expose some other protocol we recommend you use additionalResources.</p>"},{"location":"helm/built-in-chart/#additional-resources","title":"Additional resources","text":"<p>You can pass arbitrary Kubernetes resources to the Helm chart using the <code>additionalResources</code> key in the <code>values.yaml</code> file.</p> <pre><code>additionalResources:\n  - apiVersion: v1\n    kind: Secret\n    metadata:\n      name: my-secret\n    type: Opaque\n    data:\n      password: my-password\n</code></pre> <p>You can also use Helm templating in <code>additionalResources</code>.</p> <pre><code>additionalResources:\n  - apiVersion: v1\n    kind: Secret\n    metadata:\n      name: '{{ template \"agentEnv.fullname\" $ }}-secret'\n    type: Opaque\n    data:\n      password: my-password\n</code></pre> <p>For more complex resources which are not valid standalone YAML, you can use a string block. The following example creates a Cilium Network Policy which allows ingress from all entities to the default service on port 2222.</p> <pre><code>additionalResources:\n- |\n  apiVersion: cilium.io/v2\n  kind: CiliumNetworkPolicy\n  metadata:\n    name: {{ template \"agentEnv.fullname\" $ }}-sandbox-default-external-ingress\n    annotations:\n      {{- toYaml $.Values.annotations | nindent 6 }}\n  spec:\n    description: |\n      Allow external ingress from all entities to the default service on port 2222.\n    endpointSelector:\n      matchLabels:\n        io.kubernetes.pod.namespace: {{ $.Release.Namespace }}\n        {{- include \"agentEnv.selectorLabels\" $ | nindent 6 }}\n        inspect/service: default\n    ingress:\n      - fromEntities:\n        - all\n        toPorts:\n        - ports:\n          - port: \"2222\"\n            protocol: TCP\n</code></pre>"},{"location":"helm/built-in-chart/#annotations-and-labels","title":"Annotations and labels","text":"<p>You can pass arbitrary annotations and labels to the Helm chart using the top-level <code>annotations</code> and <code>labels</code> keys in the <code>values.yaml</code> file. These will be added as annotations and labels to the Pods, PVCs and network policies.</p> <pre><code>annotations:\n  my-annotation: my-value\nlabels:\n  my-label: my-value\n</code></pre> <p>The <code>k8s_sandbox</code> package automatically includes the Inspect task name as an annotation. This may be useful for determining which task a Pod belongs to.</p>"},{"location":"helm/built-in-chart/#render-chart-without-installing","title":"Render chart without installing","text":"<pre><code>helm template ./resources/helm/agent-env &gt; scratch/rendered.yaml\n</code></pre>"},{"location":"helm/built-in-chart/#manual-chart-install","title":"Install chart manually","text":"<p>Normally, the Helm chart is installed by the <code>k8s_sandbox</code> package. However, you can install it manually which might be useful for debugging.</p> <pre><code>helm install my-release ./resources/helm/agent-env -f path/to/helm-values.yaml\n</code></pre> <p><code>my-release</code> is the release name. Consider using your name or initials.</p> <p><code>./resources/helm/agent-env</code> is the path to the Helm chart. If you are outside the <code>k8s_sandbox</code> repository, pass the path to the chart in the <code>k8s_sandbox</code> repository, or the path to the chart within your virtual environment's <code>k8s_package/</code> directory.</p> <pre><code>helm install my-release ~/repos/k8s_sandbox/resources/helm/agent-env -f ...\nhelm install my-release \\\n  .venv/lib/python3.12/site-packages/k8s_sandbox/resources/helm/agent-env -f ...\n</code></pre> <p>Remember to uninstall the release when you are done.</p> <pre><code>helm uninstall my-release\n</code></pre>"},{"location":"helm/built-in-chart/#generate-docs","title":"Generate docs","text":"<p>You can regenerate docs using helm-docs after changing the default <code>values.yaml</code>.</p> <pre><code>brew install norwoodj/tap/helm-docs\nhelm-docs ./resources/helm/agent-env\n</code></pre> <p>This is done automatically by a pre-commit hook.</p>"},{"location":"helm/compose-to-helm/","title":"Automatic Docker Compose to Helm Values Translation","text":"<p>The <code>k8s_sandbox</code> package supports automatically converting Docker Compose files to Helm values files which are compatible with the built-in Helm chart at run time. This is done transparently: files won't be added to your repository.</p> <pre><code>return Task(\n    ...,\n    sandbox=(\"k8s\", \"compose.yaml\"),\n)\n</code></pre> <p>The following file names are supported for automatic translation: <code>compose.yaml</code>, <code>compose.yml</code>, <code>docker-compose.yaml</code>, <code>docker-compose.yml</code>. You must explicitly specify the relevant compose file name in the <code>sandbox</code> parameter; only <code>helm-values.yaml</code> and <code>values.yaml</code> are automatically discovered. This is to prevent unintentional translation of Docker Compose files (e.g. if your Helm values file were misnamed).</p> <p>Docker Compose files are first validated against the Compose Spec.</p>"},{"location":"helm/compose-to-helm/#rationale","title":"Rationale","text":"<p>This functionality intends to facilitate running some of the community-maintained evals which have not been (and may never be) ported to Helm <code>values.yaml</code>. Whilst it is easy to convert a <code>compose.yaml</code> file to a <code>values.yaml</code> file, it does add a maintenance burden, especially if an individual making changes in future does not have access to a Kubernetes cluster to test the changes.</p>"},{"location":"helm/compose-to-helm/#limitations","title":"Limitations","text":"<p>Only basic Docker compose functionality is supported. For more complex needs, please write a Helm values file directly.</p> <p>Images will have to be available to the Kubernetes cluster; they won't be built or pushed for you.</p> <p>For internal, non-community eval suites, native Helm <code>values.yaml</code> files are still preferred over the automatic translation of <code>compose.yaml</code> files for a number of reasons:</p> <ul> <li>To support the whole set of Helm chart and Kubernetes features</li> <li>To explicitly not support Docker for certain evals (reducing maintenance burden and   discourage use of Docker which lacks security features of Kubernetes)</li> <li>To be more expressive about which services should get a DNS entry</li> <li>To support more powerful readiness and liveness probes</li> </ul>"},{"location":"helm/compose-to-helm/#default-service","title":"Default Service","text":"<p>The default service resolution follows the same rules as Inspect sandboxing doc:</p> <p>If you define multiple sandbox environments the default sandbox environment will be determined as follows:</p> <ol> <li>First, take any sandbox environment named <code>default</code>;</li> <li>Then, take any environment with the <code>x-default</code> key set to <code>true</code>;</li> <li>Finally, use the first sandbox environment as the default.</li> </ol> <p>During conversion, services matching rules 2 or 3 are renamed to <code>default</code> to ensure consistent default service resolution regardless of Kubernetes pod ordering. For rule 2, the service with <code>x-default: true</code> is renamed. For rule 3, the \"first\" service (determined by YAML order, not alphabetical order) is renamed. Single-service compose files are left unchanged.</p>"},{"location":"helm/compose-to-helm/#internet-access","title":"Internet Access","text":"<p>As per the built-in Helm chart, internet access is disabled by default. This is in contrast to Docker Compose. There is no native way of specifying which domains should be accessible in Docker Compose. To express which domains should be accessible when running an eval in k8s, use the <code>x-inspect_k8s_sandbox</code> extension in the Docker Compose file.</p> <pre><code>services:\n  myservice:\n    image: ubuntu\nx-inspect_k8s_sandbox:\n  allow_domains:\n    - google.com\n</code></pre> <p>or</p> <pre><code>services:\n  myservice:\n    image: ubuntu\nx-inspect_k8s_sandbox:\n  allow_entities:\n    - world\n</code></pre>"},{"location":"helm/compose-to-helm/#network-modes","title":"Network Modes","text":"<p>The only supported <code>network_mode</code> is <code>none</code>, which completely isolates a service from all network traffic (both ingress and egress). This is useful for evals where the agent should not have any network access.</p> <pre><code>services:\n  isolated-service:\n    image: ubuntu\n    network_mode: none\n</code></pre>"},{"location":"helm/custom-chart/","title":"Custom Helm Chart","text":"<p>You can create your own Helm chart to be used by the <code>k8s_sandbox</code> package instead of the built-in <code>agent-env</code> chart.</p>"},{"location":"helm/custom-chart/#chart-requirements","title":"Chart requirements","text":"<ul> <li>Have one container per Pod, or if multiple containers are required on a Pod, the first   container must represent the sandbox environment in which operations will be performed   by Inspect.</li> <li>Accept a top-level <code>annotations</code> object in its values schema (will typically include   <code>annotations.inspectTaskName</code>). Apply these annotations to Pods and other resources.</li> <li>In the metadata section of each Pod, set the <code>app.kubernetes.io/instance</code> label to the   name of the Helm release i.e. <code>.Release.Name</code>. This is an 8 character string supplied   by the <code>k8s_sandbox</code> package. This is used by the <code>k8s_sandbox</code> package for   discovering the sandbox environments.</li> <li>In the metadata section of each Pod which represents a sandbox environment, set the   <code>inspect/service</code> label. The value is used for naming the sandbox environments   presented to Inspect (e.g. \"default\" or \"victim\"). Pods which do not have an   <code>inspect/service</code> label set are not presented to Inspect as sandbox environments.</li> </ul> <p>Be aware that the release name generated by the <code>k8s_sandbox</code> package may begin with a digit, so ensure that any resource names which include this result in a valid Kubernetes resource name.</p> <p>A basic chart's template file might look like this:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-custom-chart-pod-{{ .Release.Name }}\n  labels:\n    app.kubernetes.io/name: {{ .Chart.Name }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    inspect/service: default\n  annotations:\n    {{- toYaml $.Values.annotations | nindent 4 }}\nspec:\n  containers:\n  - name: default-container\n    image: python:3.12-bookworm\n    command: [\"sleep\", \"infinity\"]\n</code></pre>"},{"location":"helm/custom-chart/#using-your-chart","title":"Using your chart","text":"<p>Note</p> <p>Only local charts (i.e. ones available in a local directory) are currently supported. In future, we hope to support charts from a remote repository.</p> <p>To use the custom Helm chart, pass a <code>SandboxEnvironmentSpec</code> containing a <code>K8sSandboxEnvironmentConfig</code> as the <code>sandbox</code> parameter to the <code>Task</code> or <code>Sample</code> constructor.</p> <pre><code>Task(\n    ...\n    sandbox=SandboxEnvironmentSpec(\n        \"k8s\",\n        K8sSandboxEnvironmentConfig(\n            chart=\"path/to/your/chart\",\n            values=Path(\"path/to/your/values.yaml\"),\n        ),\n    ),\n)\n</code></pre> <p>The values can be <code>None</code> to use the chart's default values.</p>"},{"location":"helm/custom-chart/#chart-readiness","title":"Chart readiness","text":"<p>The <code>k8s_sandbox</code> package uses the <code>--wait</code> flag when installing the Helm chart so won't begin the eval until the chart is deemed ready by Helm. If this is not sufficient, consider using a Helm post-install hook which waits for a condition to be met.</p>"},{"location":"helm/helm/","title":"What is Helm?","text":"<p>Helm charts are used to deploy the sandbox environments. Under the hood, they generate Kubernetes manifests (YAML files) which are applied (installed) to your cluster.</p> <p>There is a built-in chart included with the <code>k8s_sandbox</code> package. You can also create your own Helm chart.</p> <p>Helm releases are installed in the namespace specified by the <code>current-context</code> of your kubeconfig.</p> <p>Helm is used so that researchers can write a simple <code>helm-values.yaml</code> file which Helm renders into a set of Kubernetes resources using the templates defined in the chart. This is to abstract away the complexity of writing Kubernetes resources in YAML for those who are not familiar with it.</p>"},{"location":"security/container-runtime/","title":"Container Runtime","text":""},{"location":"security/container-runtime/#gvisor","title":"gVisor","text":"<p>Using gVisor as the container runtime wherever possible is recommended for good security practice when running untrusted LLM-generated code in containers. The built-in Helm chart uses <code>gvisor</code> as the container runtime class name unless you specify otherwise (see built-in chart).</p> <p>There are however some differences in behaviour between using gVisor (which uses the <code>runsc</code> runtime) and the default <code>runc</code> runtime. This may make some Cyber misuse evals harder or impossible to solve.</p> <p>gVisor blocks certain low-level system calls such as directly creating and sending packets with <code>hashcat</code>.</p> <p>gVisor may prevent certain security vulnerabilities from being exploited, such as breaking out of chroot jails.</p> <p>gVisor may prevent agents from using password-based SSH authentication using tools like <code>sshpass</code>. They can still use key-based SSH authentication or password-based SSH authentication using tools like <code>paramiko</code>.</p> <p>Info</p> <p>To determine whether gVisor is being used as the runtime for a container or not, open a shell into the container and run <code>sudo dmesg | grep gvisor</code>. If there is a match, then gVisor is being used.</p>"},{"location":"security/network-access/","title":"Network Access","text":"<p>It is good security practice to prevent your containers from communicating with the internet by default.</p> <p>However, some evals may require internet access (e.g. to install packages or research topics). The built-in Helm chart allows you to specify a list of domains that your containers can access.</p>"},{"location":"security/network-access/#cilium","title":"Cilium","text":"<p>The built-in Helm chart uses Cilium Network Policies to restrict network access.</p> <p>Cilium has tooling to observe network requests, such as Hubble. Though note from the limitations section that domain names will not be shown when using the built-in Helm chart due to how DNS resolution is handled.</p> <p>See the limitations section for how Cilium may make certain Cyber misuse evals harder or impossible to solve.</p>"},{"location":"security/network-access/#dns-exfiltration","title":"DNS Exfiltration","text":"<p>The built-in Helm chart prevents DNS exfiltration attacks. This is where an attacker uses DNS lookups to an attacker-controlled domain or set of subdomains in order to exfiltrate data from a system. For example, a malicious agent could make DNS requests (which go via the kube-dns service if the Core DNS sidecar can't resolve them) to hostnames like <code>somedata.attacker.com</code> and <code>somedata2.attacker.com</code> to exfiltrate data.</p> <p>DNS lookups are restricted to only:</p> <ul> <li>Services within the Kubernetes namespace</li> <li>The domains specified in the <code>allowDomains</code> list in the <code>values.yaml</code> file.</li> </ul> <p>See the limitations section for how this may affect your evals.</p> I don't want to restrict DNS lookups <p>The list of DNS names that can be queried is controlled by the same allow-list as general traffic egress to a domain. In order to allow all DNS lookups without restriction (including reverse DNS), you will need to allow all traffic to the internet.</p> <p>Please consider the security implications: your containers will have unrestricted internet access (including the ability to use DNS to exfiltrate data).</p> <p>To allow all DNS queries and all internet access, set <code>allowDomains: [\"*\"]</code> or <code>allowEntities: \"all\"</code> in the <code>values.yaml</code> file.</p>"},{"location":"tips/cleanup/","title":"Cleanup","text":"<p>If the Inspect process were to terminate unexpectedly, it may leave behind resources in the Kubernetes cluster. To see if any Helm releases have been left behind, either use K9s and type <code>:helm</code> followed by enter, or use the Helm CLI:</p> <pre><code>helm list\n</code></pre> <p>To uninstall all of the Inspect-managed Helm releases:</p> <pre><code>inspect sandbox cleanup k8s\n</code></pre> <p>This will list all of the Inspect-managed Helm releases (it will infer whether they are Inspect-managed based on the labels) in the current namespace and offer to uninstall them all for you.</p> <p>Warning</p> <p>This command will find and uninstall all Inspect-managed Helm releases for any user of the Kubernetes namespace. If you are using a shared Kubernetes namespace, please be careful when choosing which Helm releases to uninstall.</p>"},{"location":"tips/concurrency/","title":"Concurrency","text":""},{"location":"tips/concurrency/#helm-install-and-uninstall-operations","title":"Helm install and uninstall operations","text":"<p>To avoid overwhelming the Kubernetes API server, the <code>k8s_sandbox</code> package limits the number of concurrent <code>helm install</code> operations to 8 and, independently, the number of <code>helm uninstall</code> operations to 8.</p> <p>Critically, there is a separate semaphore for installs and uninstalls. This prevents deadlocks if the cluster capacity has been reached.</p> <p>Inspect's console output shows the number of install and uninstall operations currently in progress.</p> <p>The maximum values can be configured by setting the <code>INSPECT_MAX_HELM_INSTALL</code> and <code>INSPECT_MAX_HELM_UNINSTALL</code> environment variables.</p> <pre><code>export INSPECT_MAX_HELM_INSTALL=10\nexport INSPECT_MAX_HELM_UNINSTALL=10\n</code></pre> <p>Do consider the effect of increasing these values on the Kubernetes API server.</p>"},{"location":"tips/concurrency/#pod-operations","title":"Pod operations","text":"<p>A pod-op is an operation that is performed on a Pod, such as <code>SandboxEnvironment</code>'s <code>exec()</code>, <code>read_file()</code>, <code>write_file()</code>. By default, this is limited on the client (i.e. the machine running the Inspect process) to CPU count * 4. You can adjust this by setting the <code>INSPECT_MAX_POD_OPS</code> environment variable.</p> <pre><code>export INSPECT_MAX_POD_OPS=100\n</code></pre> <p>These operations are typically I/O bound (from the client's perspective). Do bear in mind that these operations are routed through the Kubernetes API server.</p> <p>Inspect's console output shows the number of Pod operations currently in progress.</p>"},{"location":"tips/configuration/","title":"Advanced Configuration","text":""},{"location":"tips/configuration/#helm-install-timeout","title":"Helm install timeout","text":"<p>The built-in Helm install timeout is 10 minutes. If you're running large eval sets and expect to run into cluster capacity issues, you can increase the timeout by setting the <code>INSPECT_HELM_TIMEOUT</code> environment variable to a number of seconds.</p> <pre><code>export INSPECT_HELM_TIMEOUT=21600   # 6 hours\n</code></pre>"},{"location":"tips/configuration/#targeting-specific-or-multiple-kubeconfig-contexts","title":"Targeting specific or multiple kubeconfig contexts","text":"<p>Your kubeconfig file provides information about the Kubernetes clusters you can access.</p> <p>By default, your kubeconfig's current context is used to install Helm charts. You can determine what this is by running:</p> <pre><code>kubectl config current-context\n</code></pre> <p>At an Inspect <code>Task</code> or <code>Sample</code> level, you can specify the name of the Kubernetes context in which the Helm chart should be installed by providing a <code>K8sSandboxEnvironmentConfig</code> in the <code>sandbox</code> argument. This might be useful if for example certain Samples require GPU nodes which are only available in a specific cluster.</p> <pre><code>Sample(\n    sandbox=SandboxEnvironmentSpec(\n        \"k8s\",\n        K8sSandboxEnvironmentConfig(context=\"minikube\"),\n    ),\n)\n</code></pre> <p>Note</p> <p>If using the <code>inspect sandbox cleanup k8s</code> command, please note that it only cleans up Helm releases in the current context. Use <code>kubectl</code> or <code>k9s</code> to clean up Helm releases in other contexts.</p>"},{"location":"tips/configuration/#structured-logging-truncation-threshold","title":"Structured logging truncation threshold","text":"<p>By default, each key/value pair (e.g. an exec command's output) logged to Python's <code>logging</code> module (via structured JSON logging) is truncated to 1000 characters. This is to prevent logs from becoming excessively large when e.g. a model runs a command which produces a large amount of output. This can be adjusted by setting the <code>INSPECT_LOG_TRUNCATION_THRESHOLD</code> environment variable to a number of characters.</p> <pre><code>export INSPECT_LOG_TRUNCATION_THRESHOLD=5000\n</code></pre>"},{"location":"tips/debugging-k8s-sandboxes/","title":"Debugging K8s Sandboxes","text":"<p>This section explains features of Inspect and k9s which are particularly relevant to debugging evals which use K8s sandboxes. Please see the dedicated docs pages of each for more information.</p>"},{"location":"tips/debugging-k8s-sandboxes/#trace-log-level","title":"View Inspect's <code>TRACE</code>-level logs","text":"<p>Useful sandbox-related messages like Helm installs/uninstalls, Pod operations (<code>exec()</code> executions including the result, <code>read_file()</code>, <code>write_file()</code>) etc. are logged at the <code>TRACE</code> log level. See the Inspect tracing docs for more information on where these are stored and how to read them.</p> <p>Example (additional fields removed for brevity):</p> <pre><code>K8s installing Helm chart: {\n  \"chart\": \"/home/ubuntu/.../k8s_sandbox/resources/helm/agent-env\",\n  \"release\": \"uo4w7mvq\",\n  \"values\": \"/home/ubuntu/.../helm-values.yaml\",\n  \"namespace\": \"agent\",\n  \"task\": \"xss-attack\"\n}\n[K8s] Available sandboxes: ['default', 'default', 'victim']\nK8s execute command in Pod: {\n  \"pod\": \"agent-env-uo4w7mvq-default-0\",\n  \"task_name\": \"xss-attack\", \"cmd\": \"['python3']\",\n  \"stdin\": \"print('Hello, world!')\", \"cwd\": \"None\", \"timeout\": \"300\"\n}\n[K8s] Completed: K8s execute command in Pod. {\n  \"result\": \"ExecResult(success=True, returncode=0, stdout=\\\"...\\\", stderr\\\"\\\")\"\n  \"pod\": \"agent-env-uo4w7mvq-attacker-0\", \"task_name\": \"xss-attack\",\n  \"cmd\": \"['python3']\", \"stdin\": \"print('Hello, world!')\", \"cwd\": \"None\",\n  \"timeout\": \"300\"\n}\n</code></pre> <p>All K8s-relevant entries contain \"K8s\" as a substring within the \"action\" field which may be useful for filtering.</p> <p>The trace logs include timestamps and are invaluable when piecing together an ordered sequence of events.</p>"},{"location":"tips/debugging-k8s-sandboxes/#disabling-inspect-cleanup","title":"Disabling Inspect Cleanup","text":"<p>By default, Inspect will clean up sandboxes (i.e. uninstall Helm releases) after an eval has completed. You can pass the <code>--no-sandbox-cleanup</code> flag to prevent this, which may help with debugging. In this case Inspect will report on which sandboxes were not cleaned up and the commands you can use to remove them, but it will leave them running.</p> <p></p> <p>Note</p> <p>Unless you know there is some other automated cleanup, you should remember to uninstall your Helm releases when you're done debugging.</p>"},{"location":"tips/debugging-k8s-sandboxes/#k9s","title":"K9s","text":"<p><code>k9s</code> navigation is similar to <code>vim</code>. Use <code>:</code> to enter command mode, press <code>esc</code> to exit modes, etc.</p> <p>Start by navigating to the Pod view:</p> <ul> <li><code>:</code></li> <li>type <code>pod</code> and hit enter</li> </ul> <p></p> <p>Running an eval will cause one or more Pods to appear:</p> <p></p> <p>After highlighting a Pod with the arrow keys, the most immediately useful shortcuts are generally:</p> <ul> <li><code>s</code>: open a shell on the default container of the Pod you have currently selected. You can look around the filesystem, run commands etc. Run <code>exit</code> to close the shell when done.</li> <li><code>d</code>: describe the resource. Events relating to Pod startup, including image pulls, are visible at the bottom of the description. You'll also see <code>inspectTaskName</code> as an annotation. Press <code>esc</code> to return to the previous view.</li> <li><code>l</code>: open the logs of the default Pod container. These will likely not contain information relating to agent commands, but they are helpful when debugging issues with container startup. Press <code>esc</code> to return to the previous view.</li> <li><code>ctrl-d</code>: delete the resource. Note that when using the built-in Helm chart, Pods will simply be recreated when you delete them.</li> </ul> <p>Other resource types are also available within <code>k9s</code>. For example, press <code>:</code> and type <code>ciliumnetworkpolicies</code> (note you can tab to autocomplete) to view the network policies:</p> <p></p> <p>Describing (<code>d</code>) a network policy can be used to check which domain names should be accessible from any container in the sandbox:</p> <p></p> <p>You can use the edit shortcut (<code>e</code>) to edit a network policy (or other resources) within <code>k9s</code>. If an agent is routinely failing because it cannot make a network request, you may wish to:</p> <ul> <li>Run Inspect with the <code>--no-sandbox-cleanup</code> and <code>--log-level sandbox</code> arguments</li> <li>Make a note of the command which is failing</li> <li>Adjust (<code>e</code>) the relevant network policy as appropriate</li> <li>Shell (<code>s</code>) into the relevant Pod</li> <li>Manually re-run the commands</li> </ul> <p><code>k9s</code> also exposes a <code>helm</code> resource which lists Helm releases in the currently selected namespace. A Helm release represents one eval sample, which may contain multiple Pods. To view all Helm charts, press <code>:</code> then type <code>helm</code> and hit enter. You can delete (<code>ctrl-d</code>) to uninstall releases.</p> <p></p>"},{"location":"tips/hubble/","title":"Hubble (Cilium UI)","text":"<p>If you're using the built-in Helm chart, Cilium is being used for network policy enforcement. Cilium includes a UI called Hubble which can be used to monitor network traffic.</p> <p></p> <p>This is particularly useful for curating a list of allowed domain names for your evals.</p> <p>Prerequisites</p> <p>You'll need access to the namespace in which <code>hubble-ui</code> is deployed (<code>kube-system</code> by default).</p> <p>You'll need to enable the Hubble UI in your Cilium installation if it's not already enabled.</p> <p>To access Hubble, install the <code>cilium</code> CLI on your development machine and run:</p> <pre><code>cilium hubble ui\n</code></pre> <p>Then you can access Hubble in your browser at http://localhost:12000.</p> <p>Stuck on the loading page?</p> <p>Try changing the port number. To forward to a random port number:</p> <pre><code>cilium hubble ui --port-forward 0\n</code></pre>"},{"location":"tips/images/","title":"Docker Images","text":"<p>Any Docker images required by your evals will need to be available to the cluster.</p>"},{"location":"tips/images/#best-practices","title":"Best practices","text":"<p>A Docker image built from source (i.e. a Dockerfile) may change over time. For reproducibility, it is recommended to build, tag and push the image to a registry.</p> <p>In your <code>values.yaml</code> file, you should specify a tag (i.e. version) for the image rather than the implicit <code>latest</code>.</p> <p>A registry contains repositories for each image, each with multiple tags for different versions.</p> <p>It is best practice to configure the repositories to have tag immutability i.e. once a tag (e.g. <code>1.0.0</code>) is pushed, it cannot not be overwritten.</p>"},{"location":"tips/images/#aws","title":"AWS","text":"<p>If using AWS infrastructure, you might choose ECR as your image registry.</p> <p>To authenticate with ECR, you can use the following command, replacing the <code>&lt;aws_acc_id&gt;</code> with your AWS account ID and adjusting the region as necessary:</p> <pre><code>aws ecr get-login-password --region eu-west-2 | \\\n    docker login --username AWS --password-stdin \\\n    &lt;aws_acc_id&gt;.dkr.ecr.eu-west-2.amazonaws.com\n</code></pre> <p>To create a new repository in ECR:</p> <pre><code>aws ecr create-repository --repository-name &lt;repository-name&gt; \\\n    --image-scanning-configuration scanOnPush=true \\\n    --image-tag-mutability IMMUTABLE\n</code></pre> <p>To build your image and push it to ECR:</p> <pre><code>IMAGE=&lt;aws_acc_id&gt;.dkr.ecr.eu-west-2.amazonaws.com/&lt;repository-name&gt;:&lt;version&gt;\ndocker build -t $IMAGE .\ndocker push $IMAGE\n</code></pre>"},{"location":"tips/troubleshooting/","title":"Troubleshooting","text":"<p>For general K8s and Inspect sandbox debugging, see the Debugging K8s Sandboxes guide.</p>"},{"location":"tips/troubleshooting/#view-inspects-trace-level-logs","title":"View Inspect's <code>TRACE</code>-level logs","text":"<p>A good starting point to many issues is to view the <code>TRACE</code>-level logs written by Inspect. See the <code>TRACE</code> log level section.</p>"},{"location":"tips/troubleshooting/#helm-context-deadline-exceeded","title":"I'm seeing \"Helm install: context deadline exceeded\" errors","text":"<p>This means that the Helm chart installation timed out. When installing the Helm chart, the <code>k8s_sandbox</code> package uses the <code>--wait</code> flag to wait for all Pods to be ready.</p> <p>Therefore, this error can be an indication of:</p> <ul> <li>If you have an auto-scaling cluster, it may need more time to provision new nodes.</li> <li>If you don't have an auto-scaling cluster, you may have reached capacity.</li> <li>If you are using large images, they may take a long time to pull onto the nodes.</li> <li>A Pod failing to enter the ready state (could be a failing readiness probe, failing to   pull the image, crash loop backoff, etc.)</li> </ul> <p>Consider increasing the timeout.</p> <p>If your cluster does not auto-scale and it is at capacity, consider reducing parallelism or scaling up the relevant cluster node group.</p> <p>Try installing the chart again (this can also be done manually) and check the Pod statuses and events using a tool like kubectl or K9s to get a definitive answer as to the underlying problem. Use the Helm release name (will be in error message) to filter the Pods.</p>"},{"location":"tips/troubleshooting/#im-seeing-helm-uninstall-failed-errors","title":"I'm seeing \"Helm uninstall failed\" errors","text":"<p>The <code>k8s_sandbox</code> package ignores \"release not found\" errors when uninstalling Helm releases because they are expected when the Helm release was not successfully installed (including when the user cancelled the eval).</p> <p>Other uninstall failures (e.g. \"failed to delete release\") will result in an error.</p> <p>Check to see if any Helm releases were left behind:</p> <pre><code>helm list\n</code></pre> <p>And if you wish to uninstall them:</p> <pre><code>helm uninstall &lt;release-name&gt;\n</code></pre> <p>If you wish to bulk uninstall all Inspect Helm charts, see the cleanup command.</p>"},{"location":"tips/troubleshooting/#im-seeing-handshake-status-404-not-found-errors-from-pod-operations","title":"I'm seeing \"Handshake status 404 Not Found\" errors from Pod operations","text":"<p>This typically indicates that the Pod has been killed. This may be due to:</p> <ul> <li>cluster issues (see View cluster events)</li> <li>because the eval had already failed for an unrelated reason and the Helm releases were   uninstalled whilst some operations were queued or in flight. Check the <code>.json</code> or   <code>.eval</code> log produced by Inspect to see the underlying error.</li> </ul>"},{"location":"tips/troubleshooting/#view-cluster-events","title":"View cluster events","text":"<p>Certain cluster events may impact your eval, for example, a node failure.</p> <p>The following commands are a primitive way to view cluster events. Your cluster may have observability tools which collect these events and provide a more user-friendly interface.</p> <pre><code>kubectl get events --sort-by='.metadata.creationTimestamp'\n</code></pre> <p>To also see timestamps:</p> <pre><code>kubectl get events --sort-by='.metadata.creationTimestamp' \\\n  -o custom-columns=LastSeen:.lastTimestamp,Type:.type,Object:.involvedObject.name,Reason:.reason,Message:.message\n</code></pre> <p>To filter to a particular release or Pod, either pipe into <code>grep</code> or use the <code>--field-selector</code> flag:</p> <pre><code>kubectl get events --sort-by='.metadata.creationTimestamp' \\\n  --field-selector involvedObject.name=agent-env-xxxxxxxx-default-0\n</code></pre> <p>Find the Pod name (including the random 8-character identifier) in the <code>TRACE</code>-level logs or the stack trace.</p> <p>To specify a namespace other than the default, use the <code>-n</code> flag.</p>"}]}